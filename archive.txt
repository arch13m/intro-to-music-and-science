

### Levels of measurement

What kinds of variables might we have in music psychology experiments? There are many possibilities. Most can usually be categorised as either *participant variables* or *stimulus variables*.

-   **Participant variables.** These are properties of the *participant*. Many of these are personal traits that existed before the experiment, for example age, gender, prior musical training, and so on. Other of these might be variables that are determined during the experiment, for example when the experimenter assigns the participants to different experimental conditions. For example, suppose that we conduct a study on schoolchildren, investigating the effect of music lessons on intelligence; we might end up with the following table, where each row is a participant, and each column is a participant variable:

| Name   | Gender | Start age | Condition     | Start IQ | End IQ |
|--------|--------|-----------|---------------|----------|--------|
| Oliver | Male   | 12        | No lessons    | 110      | 111    |
| George | Male   | 11        | Music lessons | 105      | 110    |
| Olivia | Female | 11        | No lessons    | 99       | 98     |
| Amelia | Female | 12        | Music lessons | 103      | 104    |

: Fictional dataset illustrating different participant variables.

-   **Stimulus variables.** These are properties of a *stimulus*. A given music psychology experiment might involve many different musical stimuli, and we might be interested in how different stimuli elicit different responses from participants. Below is an excerpt from a real published dataset [@Bowling2018-hz] of 'attractiveness' ratings for musical chords, supplemented with computational analyses of these chords from a later publication [@Harrison2020-gx]:

```{r}

readRDS("input/consonance/perception-bowling.rds") %>% 
  transmute(
    # Chord = Hmisc::capitalize(name), 
    `Interval (semitones)` = map_chr(pi_chord, ~ paste(sprintf("%.2f", diff(.)), collapse = ", ")),
    # Pitches = map_chr(pi_chord, ~ paste(sprintf("%.2f", .), collapse = ", ")),
    `Attractiveness rating` = sprintf("%.2f", rating),
    Roughness = sprintf("%.2f", hutch_78_roughness),
    Harmonicity = sprintf("%.2f", har_18_harmonicity),
    Familiarity = sprintf("%.2f", - scale(har_19_corpus))
  ) %>% 
  slice(1:12) %>% 
  knitr::kable(caption = "Attractiveness ratings for dyads from @Bowling2018-hz supplemented with computational analyses from @Harrison2020-gx.")
```

When we collect scientific data, we generally wish to attribute some meaning to this data that goes beyond the immediate mechanism by which the data were generated. For example, if we collect multiple-choice 'pleasantness' ratings for musical stimuli, this is probably because we want to get a handle on the underlying pleasantness of these stimuli. In other words, we have operationalised pleasantness in terms of 

Creating scientific variables is essentially a reductive process. We are taking a complex phenomenon, for example personality, and operationalising it in terms of a mechanistic measurement procedure that generates (most commonly) one or more numbers as an output. This process of operationalisation is highly valuable in that it allows us downstream to apply a whole range of quantitative data analysis techniques. However, it's dangerous in that flaws in our operationalisations can cause us to draw highly misleading conclusions from our datasets. It is therefore essential to maintain a critical perspective on variable definitions at all times, both when conducting one's own studies and when reading other studies in the literature.

The challenge of psychological measurement

It is relatively easy to define a valid measurement process for many physical variables, for example height and weight. You can measure someone's height with a tape measure, and you can measure their weight with a weighing scale. Sure, if you're doing it properly you might have to think carefully about some subtle points of the methodology (should the height measurement include the person's hair? does it matter whether we measure the person in the morning versus the evening, or before or after a large meal?), but conceptually it is not too difficult to convince oneself of the validity of the measurement process.

In contrast, it is relatively complicated to define a valid measurement process for many psychological variables, because these variables are generally much more theoretical in their conceptualisation. Take personality as an example.

It's certainly quick and efficient to measure someone's personality using just 10 multiple-choice questions. The question is, though, how 'good' is the resulting measurement? In psychology we typically answer that question by thinking about two subcomponents of measurement quality: reliability and construct validity.

A clear limitation of this questionnaire

Certain behavioural measures can be questioned for their validity. For example, if we play people

it truly measures the underlying trait which it is purported to measure [@Cronbach1955-ep].

It is generally accepted that establishing construct validity requires the researcher to engage explicitly with theoretical aspects of the trait and its measurement process; in other words, construct validity cannot be established in a theory-free manner. This differentiates it from reliability, which can typically be established without any kind of reference to theory (e.g. by administering the same test twice to the same participants and correlating the results).

Theorising about construct validity requires the researcher to be particularly clear about two criteria:

How exactly do we define the trait that we are trying to measure?

How should this trait manifest in the observable world?

Different theorists have proposed different routines for establishing construct validity. There is still no real consensus on the precise procedures to carry out, though, and in practice construct validity establishment is often dealt with in a somewhat ad hoc, case-by-base manner.

Psychological texts often differentiate various kinds of construct validity. These represent different strategies that a researcher might use to substantiate the construct validity of a given measurement instrument. Here are several particularly important examples:

Concurrent validity

Concurrent validity is established when a new measure is shown to correlate highly with a pre-established 'gold-standard' measure. This gives us confidence that, if the old measure was valid, then the new measure is also valid, because it produces similar results. This kind of validity is useful when developing new forms of tests for new kinds of testing environments; for example, the personality questionnaire described above (the TIPI [@Gosling2003-bt]) was developed to replicate the results from pre-existing personality questionnaires but in a particularly short and efficient manner. However, it is not very useful when trying to establish a completely new measurement instrument, in which case there is no pre-existing reference instrument to work with.

Convergent validity

Convergent validity is established when a new measure is shown to correlate positively and substantially with certain other measures as predicted by theoretical considerations. The difference between convergent validity and concurrent validity is that these other measures are not meant to be measuring the same trait as the new measure; rather, they are meant to be measuring related traits. For example, suppose that I try to validate a new musical ability questionnaire by correlating its results with scores from a melodic dictation exercise. In general, I would expect participants with higher musical ability to score higher on the melodic dictation exercise, and so a positive correlation between the two scores would provide some support

- gold-standard

Convergent validity. - correlates with things it's meant to correlate with

Discriminant validity.

Predictive validity. – predicting future outcomes

Unfortunately there is some discrepancy in the literature about the definitions of these different terms. This is somewhat inevitable given the fuzziness of the underlying topic. Do not worry too much about this; the important thing is that these different terms provide useful starting points for conversations about interpreting psychological measures.

Prediction

In some situations we are interested in learning to predict a certain variable from another collection of variables. For example, in a music education context, we might be interested in predicting which of the new intake of students are likely to do well in their end-of-degree exams. We could compile various kinds of variables for these students — age, gender, pitch discrimination skill, melodic dictation skill, piano performance skill, musicology exam results — and develop a model that predicts final outcomes on the basis of all of these variables. 

The primary challenge in quantitative research is to characterise relationships between variables. Scientists look in particular for two kinds of relationships: 

Prediction. When we say that variable X predicts variable Y, this means that if you tell me the value of variable X, I can give a good estimate of the value of variable Y.

Causation. When we say that variable X causally affects variable Y, this means that if I expressly change variable X in a particular manner, I should elicit a predictable effect on variable Y.

~~~~

# Introduction

Humans are natural experimenters. From a very young age, children naturally learn about the world through experimentation. Watching a young child play, you can just imagine the kinds of questions going through their heads: I wonder what sound it'll make if I hit this block on that pipe? I wonder what that bug tastes like? I wonder what will happen if I flush my toy car down the toilet?

These kinds of intuitive experiments are typically personal and *ad hoc*. You try something for yourself (e.g. eating a bug), you gain some kind of information (e.g. bugs taste gross), and you learn something for the future (don't eat bugs).

Science uses experiments to build up coherent theories about how the world works. It is fundamentally a *cumulative* endeavour: each scientist builds on the results generated by previous scientists.

> If I have seen further it is by standing on the shoulders of giants (Newton, 1676).

Experiments need certain qualities in order to be useful to future scientists. These qualities have gradually become canonised within the scientific value system, and are essential for work to be truly valued within the scientific community:

-   **Replicability.** When describing an experiment, we are obliged to present sufficient details such that another scientist can come along and repeat that same experiment. These details are essential for enabling other researchers to verify the *results* of the experiment by repeating it themselves. Such studies are termed *replication studies*, and are becoming an increasingly important and valued part of psychology research. These methodological details are also essential for enabling other researchers to verify the *conclusions* drawn from the experiment's results; it is often only when we investigate the details of the methodology that we end up spotting potential alternative explanations for a given pattern of results.

-   **Objectivity.** The intuitive experiments that a child performs to explore the world are naturally subjective; many of them rely after all on exploring the *sensations* delivered by objects (sound, taste, touch, ...). Subjective feelings are very valuable to the individual, but they are less meaningful to others, because different individuals by nature will have different subjective appraisals for a given physical phenomenon. Science therefore places high value on *objectivity*, the sense in which a piece of evidence is independent of the idiosyncrasies of the individual experimenter. Scientific measurements are therefore typically conducted using highly explicit methodologies, for example measuring length with a ruler, measuring voltage with a voltmeter, or assessing personality types with a questionnaire.

-   **Generalisability.** For an experiment to be relevant to other scientists, the experiment must be usefully *generalisable*. This can mean many things. For example, if we conduct a psychology experiment in the Centre for Music of Science, playing undergraduate participants' particular sounds and recording their responses, we might want to be convinced that we'd get analogous responses if we conducted the same experiment in the Department of Psychology at Goldsmiths using non-students recruited from the general London population. The more generalisable our results, the more convinced we can be that we've learned something general about human nature, rather than something specific about our particular experimental setup or our particular experiment group.

Designing an effective research programme that satisfies all of these principles while also telling us something useful is a challenging task, but it is also creative and rewarding. The next part of this course will go through this process in some detail.
