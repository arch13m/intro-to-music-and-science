# Computational music psychology

```{r, include = FALSE}
library(tidyverse)
library(ggpubr)
theme_set(theme_pubr())
source("setup.R")
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)

```

Computational music psychology is a kind of music psychology that relies heavily on computer models. These computer models are special algorithms that simulate certain acoustic, biological, and/or mental processes thought to be involved in particular musical behaviours. Used correctly, they can form an integral part of the scientific process.

## Why use computer models?

Computer models can bring several important benefits to music psychology:

**Addressing woolliness.** When we write about psychological theories with words, it's easy for ambiguities to slip in, making our arguments 'woolly'. In contrast, if we implement our theory as a computational model, we are forced to be completely explicit about what we mean. In the process, we may uncover important unwritten assumptions or logical gaps in our theory that deserve addressing.

**Enhancing testability.** A good scientific theory should be good at predicting observed phenomena. In order to test this, we must generate predictions from the theory in various empirical scenarios. When our theory is only specified verbally, it can be difficult to work out just what a theory predicts in a given scenario; conversely, once we've collected empirical data, it can be difficult to work out exactly what these data mean for the theory. Computational modelling addresses these problems. The model directly generates numeric predictions for different empirical scenarios, which can then be related to empirical data using standard statistical methods. In music psychology research, this feature is especially valuable for helping us to run psychological experiments using realistic music rather than basic artificial stimuli.

**Creating useful tools.** Suppose we end up producing a computational implementation of a particular cognitive model. The original motivation of producing this computational model may have been to obtain testable predictions for upcoming behavioural experiments. However, as a byproduct we have obtained a piece of software that is capable of solving the particular cognitive task that we are studying. Depending on the cognitive task and the success of the implementation, this can be a useful outcome in itself. For example, suppose that we have implemented a cognitive model for recognising emotion in melodies; we could then use a software implementation of this model for applications such as automated playlist generation.

## What do music psychology models look like?

The literature contains many different computational models of music perception and production (see [@Temperley2013-fg] for a review). We will begin by introducing a couple of particularly prominent such models: the Krumhansl-Schmuckler key-finding algorithm [@krumhansl1990cognitive], and the Hutchinson-Knopoff dissonance algorithm [@hutchinson1978].

### The Krumhansl-Schmuckler key-finding algorithm

#### Overview

This algorithm is designed to simulate how listeners identify the *key* of a given musical passage. Now, identifying keys is often trivial to someone reading a musical score. Many musical pieces have their key written in their title, for example "Sonata No. 6 in D major, K. 284" or "Symphony No. 9 in D minor, Op. 125". Even if the key is not present in the title, we can typically narrow down the key to one of two options by inspecting the key signature at the beginning of the piece. However, none of this written information is necessarily available to the music *listener*; they must instead somehow identify the key purely from the notes that they hear.

The Krumhansl-Schmuckler algorithm proposes a mechanism for this process. The algorithm can be succinctly summarised as follows:

1.  Different keys use different pitch classes to different degrees.[^computational-music-psychology-1]

2.  Listeners possess internal templates that summarise the pitch-class distribution for different musical keys.

3.  During a musical piece, listeners track the distribution of pitch classes within that piece. The inferred key then corresponds to the key whose pitch-class distribution matches best to the observed distribution.

[^computational-music-psychology-1]: Pitch classes denote sets of pitches separated by whole numbers of octaves. In the Western 12-tone scale, there are exactly 12 pitch classes: C, C\#, D, D\#, E, F, F\#, G, G\#, A, A\#, and B.

#### Implementation details

In order to implement this as a computer model, we need to fill in a few details.

First, we ask, what exactly do the listeners' templates look like? In the original Krumhansl-Schmuckler algorithm, these templates are derived empirically from behavioural experiments using Krumhansl and Kessler's [@krumhansl1982] *probe-tone* paradigm, where the listener is asked to rate the 'fit' of a particular musical note in a particular musical context. These experiments produced the following templates:

```{r}
kk_profiles <- list(
  # This list contains two profiles: one for the major mode and one for the minor mode.
  Major = c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88),
  Minor = c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
)
```

```{r, echo = FALSE, out.width = "100%"}
kk_profiles %>% 
  as_tibble() %>% 
  mutate(chromatic_scale_degree = 0:11) %>% 
  pivot_longer(cols = c("Major", "Minor"), names_to = "mode", values_to = "rating") %>% 
  ggplot(aes(chromatic_scale_degree, rating, colour = mode)) + 
  geom_line() + 
  scale_x_continuous("Interval above the tonic (semitones)", breaks = 0:12) + 
  scale_y_continuous("Fit") +
  scale_colour_manual(values = c("dodgerblue3", "darkgoldenrod1")) +
  facet_wrap(~ mode, ncol = 1) +
  theme(
    legend.position = "none",
    panel.grid.major.x = element_line(colour = "grey87"),
    strip.background = element_blank(),
    strip.text = element_text(face = "bold"),
    panel.spacing = unit(3, "lines")
  )
```

In the above figure, pitch classes are expressed as intervals above the tonic, written in semitones. This means for example that the tonic corresponds to 0 semitones, the major third corresponds to 4 semitones, and the perfect fifth to 7 semitones.

Note how the major template exhibits high fit for pitch classes from the major scale (0, 2, 4, 5, 7, 9, 11), and low fit for the other pitch classes (1, 3, 6, 8, 10). Similarly, the minor template exhibits high fit for pitch classes from the minor scale (0, 2, 3, 5, 7, 8) and low fit for the others.

To derive the pitch-class template for a given musical key, we need to replot the horizontal axis in terms of absolute pitch classes rather than intervals above the tonic. To achieve this transformation, we simply take the interval and add it onto the tonic. For example, if we are in F major, then an interval of 0 semitones corresponds to F, an interval of 1 semitone corresponds to F\#, and so on.

The following figure illustrates the resulting pitch-class profiles for different tonics, with the tonic marked as a dashed red line. As the tonic moves upwards, the pitch-class profile 'rotates' along with it.

```{r, echo = FALSE, out.width = "100%", animation.hook = "gifski"}
for (tonic in 0:11) {
  pcs <- c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B")
  p <- 
    kk_profiles %>% 
    as_tibble() %>% 
    mutate(chromatic_scale_degree = 0:11) %>% 
    pivot_longer(cols = c("Major", "Minor"), names_to = "mode", values_to = "rating") %>%
    mutate(
      pitch_class = (tonic + chromatic_scale_degree) %% 12
    ) %>%
    ggplot(aes(pitch_class, rating, colour = mode)) + 
    geom_vline(xintercept = tonic, colour = "red", linetype = "dashed") +
    geom_line() + 
    scale_x_continuous(
      "Pitch class", 
      breaks = 0:12,
      sec.axis = sec_axis(
        trans = identity,
        breaks = 0:11,
        labels = pcs
      )
    ) + 
    scale_y_continuous("Fit") +
    scale_colour_manual(values = c("dodgerblue3", "darkgoldenrod1")) +
    facet_wrap(~ mode, ncol = 1) +
    theme(
      legend.position = "none",
      panel.grid.major.x = element_line(colour = "grey87"),
      strip.background = element_blank(),
      strip.text = element_text(face = "bold"),
      panel.spacing = unit(3, "lines"),
      title = element_text(face = "bold")
    ) + 
    ggtitle(paste0("Tonic: ", pcs[1 + tonic]))
  print(p)
}
```

So, we have now established what the templates look like for the 24 major and minor keys. There remains one question, however: how do we compare the listener's pitch-class distribution to these template distributions? There are various possibilities here [@albrecht2013], but the original Krumhansl-Kessler algorithm uses a standard statistic called the *Pearson correlation*. We don't need to worry about the mathematical details here. The important thing to know is that the Pearson correlation can be interpreted as a measure of *similarity*, and it takes values between -1 (maximally dissimilar) and 1 (maximally similar).

#### Worked example

Let's now see this algorithm applied in practice. We'll take the well-known lullaby 'Twinkle, Twinkle, Little Star', which looks like the following in traditional music notation:

![Twinkle, Twinkle, Little Star. Credit: Helix84, [CC BY-SA 3.0](http://creativecommons.org/licenses/by-sa/3.0/), via Wikimedia Commons.](images/twinkle-twinkle.png){width="100%"}

Our first step is to translate this melody into machine-readable notation. We can write it in tabular format, where the first column is the pitch (expressed as a MIDI note), and the second column is the duration (expressed in crotchets, or quarter notes).

```{r}
twinkle <- tribble(
  ~ pitch,  ~ duration,
  60, 1,
  60, 1,
  67, 1,
  67, 1,
  69, 1,
  69, 1,
  67, 2,
  
  65, 1,
  65, 1,
  64, 1, 
  64, 1,
  62, 1,
  62, 1, 
  60, 2,
  
  67, 1, 
  67, 1,
  65, 1,
  65, 1,
  64, 1, 
  64, 1,
  62, 2,
  
  67, 1, 
  67, 1,
  65, 1,
  65, 1,
  64, 1, 
  64, 1,
  62, 2,
  
  60, 1,
  60, 1,
  67, 1,
  67, 1,
  69, 1,
  69, 1,
  67, 2,
  
  65, 1,
  65, 1,
  64, 1, 
  64, 1,
  62, 1,
  62, 1, 
  60, 2,
)
twinkle %>%
  set_names(c("Pitch", "Duration")) %>%
  DT::datatable(
    options = list(
      searching = FALSE,
      rowId = FALSE
    ))
```

We then compute the relative durations of each pitch class in the melody. To convert pitches to pitch classes we divide by 12 and take the remainder; for example, the pitch 67 is equal to 12 \* 5 + 7, so its pitch class is 7. Applying this logic, and expressing durations as a percentage of the total melody duration, we get the following:

```{r, out.width = "100%"}
twinkle_profile <- 
  twinkle %>%
  mutate(
    pitch_class = factor(pitch %% 12, levels = 0:11)
  ) %>%
  group_by(pitch_class, .drop = FALSE) %>%
  summarise(
    duration = sum(duration)
  ) %>%
  mutate(
    pitch_class = as.numeric(as.character(pitch_class)),
    rel_duration = duration / sum(duration)
  )

twinkle_profile %>%
  ggplot(aes(pitch_class, 100 * rel_duration)) + 
    geom_line(colour = "dodgerblue3") + 
    scale_x_continuous(
      "Pitch class", 
      breaks = 0:12,
      sec.axis = sec_axis(
        trans = identity,
        breaks = 0:11,
        labels = pcs
      )
    ) + 
  scale_y_continuous("Relative duration (%)")
```

Now our task is to compare this profile with all the different reference profiles, to work out which provides the best fit. The following animation evaluates each reference profile in turn (top panel) and displays the resulting correlation (bottom panel).

```{r, out.width = "100%", animation.hook = "gifski", interval = 1.5, fig.height = 7}
get_kk_profile <- function(mode, tonic) {
  ref_profile <- kk_profiles[[mode]]
  absolute_pitch_classes <- 0:11
  relative_pitch_classes <- (absolute_pitch_classes - tonic) %% 12
  ref_profile[1 + relative_pitch_classes]
}

kk_all_key_profiles <- map_dfr(0:11, function(tonic) {
  c("Major", "Minor") %>% 
    set_names(., .) %>% 
    map_dfr(function(mode) {
      tibble(
        tonic = tonic,
        mode = mode,
        profile = list(get_kk_profile(mode, tonic))
      )
    })
}) %>%
  arrange(mode)

kk_correlations <- 
    kk_all_key_profiles %>% 
    mutate(
      correlation = map_dbl(
        profile,
        cor, 
        twinkle_profile$rel_duration
      ))

.tmp <- pmap(kk_all_key_profiles, function(tonic, mode, profile) {
  tonic_label <- pcs[tonic + 1]
  key_label <- paste0(tonic_label, " ", mode)
  
  plot_profile <- 
    bind_rows(
      twinkle_profile |> 
        select(pitch_class, rel_duration) |> 
        mutate(group = "Twinkle twinkle"),
      tibble(
        pitch_class = 0:11,
        rel_duration = profile / sum(profile),
        group = key_label
      )
    ) |> 
    mutate(
      group = factor(group, levels = c("Twinkle twinkle", key_label))
    ) |> 
    ggplot(aes(pitch_class, rel_duration * 100, colour = group)) +
    geom_vline(xintercept = tonic, colour = "red", linetype = "dashed") +
    geom_line() + 
    scale_x_continuous(
      "Pitch class", 
      breaks = 0:12,
      sec.axis = sec_axis(
        trans = identity,
        breaks = 0:11,
        labels = pcs
      )
    ) + 
    scale_y_continuous("Relative duration (%)") +
    scale_colour_manual(NULL, values = c("dodgerblue3", "darkgoldenrod1")) +
    ggtitle(
      paste0("Key: ", key_label)
    ) +
    theme(
      legend.position = "right",
      plot.title = element_text(
        face = "bold"
      )
    )
  
  plot_all_correlations <- 
    kk_correlations |> 
    mutate(selected = (tonic == !!tonic & mode == !!mode)) |> 
    ggplot(aes(tonic, correlation,
               colour = mode, 
               shape = selected,
               size = selected)) + 
    geom_point() + 
    scale_x_continuous(
      "Tonic", 
      breaks = 0:12,
      sec.axis = sec_axis(
        trans = identity,
        breaks = 0:11,
        labels = pcs
      )
    ) +
    scale_y_continuous("Pearson correlation", limits = c(-1, 1)) +
    scale_shape_manual(values = c(1, 16), guide = "none") + 
    scale_size_manual(values = c(2, 5), guide = "none") + 
    scale_colour_manual(NULL, values = c("dodgerblue3", "darkgoldenrod1")) + 
    theme(
      legend.position = "right"
    )
  egg::ggarrange(
    plot_profile,
    plot_all_correlations,
    ncol = 1
  ) |> 
    print()
})

```

The results can be expressed in a table like the following. You can click on particular column headers to sort by the values in that column.

```{r}
kk_correlations |> 
  mutate(
    tonic = plyr::mapvalues(tonic, from = 0:11, to = pcs),
    correlation = sprintf("%.2f", correlation)
  ) |> 
  select(
    Tonic = tonic,
    Mode = mode,
    Correlation = correlation
  ) |> 
  DT::datatable(
    options = list(
      searching = FALSE
    )
  )
```

If we sort the 'Correlation' column by descending values, we see the following top three key candidates: C major, F major, and G major. Conversely, if we sort the column by ascending values, we see the following bottom three candidates: F\# major, A minor, and C\# major. So, the algorithm selects C major as the inferred key for the melody, which happily is consistent with what most music theorists would say if they examined the score.

#### Where should the templates come from?

The original Krumhansl-Schmuckler algorithm uses template profiles derived from behavioural experiments using the probe-tone paradigm. The implementation leaves open the question of where exactly these templates come from.

One possibility is that the templates are learned through exposure to particular musical styles. By hearing pieces in particular musical keys, the listener learns that particular keys tend to be associated with particular pitch-class distributions. This learning process is complicated by the fact that the listener has to learn the key profiles without being told what key individual pieces are in. Inspiration for understanding this process can come from the machine-learning field of *unsupervised learning*, which studies algorithms for learning structure from unlabelled data. In particular, several key-finding algorithms have used a technique called *latent Dirichlet allocation* to learn both key labels and key profiles directly from unlabelled musical corpora [@moss2021; @Hu2009].

#### Limitations

The Krumhansl-Schmuckler algorithm has been very influential within the field of music psychology, and many subsequent key-finding algorithms use a similar kind of template-matching process [@albrecht2013; @Bellmann2005; @Sapp2011; @Temperley1999a]. Nonetheless, the algorithm is limited by the fact that it ignores the *order* in which notes are played, despite psychological evidence that note order may indeed matter for listeners' key judgments [@brown1988; @matsunaga2005].

For illustration, consider the following two tone sequences from @matsunaga2005. Both of these sequences use exactly the same pitch classes, just in a different order, and with one of the pitch classes transposed upwards by an octave. The first sequence elicits a very clear tonal centre:

```{r, results = "asis"}
embed_image_with_audio(
  image = "images/matsunaga-tonally-clear-1.svg",
  audio = "audio/matsunaga-tonally-clear.mp3",
  width = "100%",
  title = "A tone sequence with very clear tonal implications, from @matsunaga2005."
)
```

In contrast, the second tone sequence elicits much more ambiguous tonal centre:

```{r, results = "asis"}
embed_image_with_audio(
  image = "images/matsunaga-tonally-ambiguous-1.svg",
  audio = "audio/matsunaga-tonally-ambiguous.mp3",
  width = "100%",
  title = "A tone sequence with ambiguous tonal implications, from @matsunaga2005."
)
```

@matsunaga2005 studied these two sequences alongside 58 others with the same pitch classes, and demonstrated that they elicit wide variations in key judgments. This means that the Krumhansl-Schmuckler algorithm cannot be capturing the full story, then.

Consequently, an interesting avenue of future research concerns extending the Krumhansl-Schmuckler algorithm to incorporate sensitivity to tone order. One such solution was trialled by @Toiviainen2003a, based on a dataset of relatedness judgments for pairs of tones in major and minor key contexts [@krumhansl1990cognitive]; however, this version of the algorithm failed to predict listener behaviour better than the original Krumhansl-Schmuckler algorithm. Not long later, @Madsen2007 presented a version of the Krumhansl-Schmuckler algorithm based on distributions of pairs of scale degrees as observed in music corpora, but found only marginal improvements in the algorithm's ability to recover musical keys from annotated music corpora. These failures to improve significantly upon the Krumhansl-Schmuckler algorithm are interesting, and suggest that we still have more to learn about key-finding.

### R implementation

See below for a full implementation of the Krumhansl-Schmuckler algorithm, written for the R programming language. This is provided for interest only, you are not expected to understand it, especially if you have never used R before! However, it is worth spending a while reading through the code line by line to try and get a feel for what is happening at each step. If you are confused about what a particular function does, you can click on its name to see its documentation.

To explore this code further, you can follow [this link](https://colab.research.google.com/drive/1tFJWKTjXs0qNR3eIXwR-sTsEuUOC1img#scrollTo=08Pjk6Oz9m85) to access an online interface that allows you to edit and run the code interactively in your web browser. You will need a (free) Google account to continue; alternatively, Cambridge students can simply sign in using their Cambridge email address.

```{r, echo = TRUE}
# Load dependencies (packages which
# provide us with some useful functions).
library(dplyr)
library(purrr)

# We write out the Krumhansl-Kessler probe tone profiles.
# There are two such profiles:
# one for the major mode and one for the minor mode.
kk_profiles <- list(
  Major = c(
    6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 
    2.52, 5.19, 2.39, 3.66, 2.29, 2.88
  ),
  Minor = c(
    6.33, 2.68, 3.52, 5.38, 2.60, 3.53,
    2.54, 4.75, 3.98, 2.69, 3.34, 3.17
  )
)

# We define a function for deriving the Krumhansl-Kessler 
# profile for a given mode and tonic.
get_kk_profile <- function(mode, tonic) {
  # We begin by extracting the reference profile for that 
  # particular mode.
  ref_profile <- kk_profiles[[mode]]
  
  # We want our profile to give the fit ratings for the 
  # absolute pitch classes 0 to 11.
  absolute_pitch_classes <- 0:11
  
  # To express these pitch classes relative to the provided
  # tonic, we subtract the tonic and apply the modulo 12
  # operator (i.e. remainder after division by 12).
  relative_pitch_classes <- (absolute_pitch_classes - tonic) %% 12

  # This tells us where to look in the reference profile.
  ref_profile[1 + relative_pitch_classes]
}

# We now compile a table of all the possible key profiles,
# produced by rotating the original Krumhansl-Kessler key profiles
# through all 12 possible tonics.

# This code iterates over the numbers 0 to 11, 
# setting the tonic first to 0, then to 1, then to 2, 
# and so on...
kk_all_key_profiles <- map_dfr(0:11, function(tonic) {
  # This code iterates over the two modes (major and minor)...
  c("Major", "Minor") %>% 
    set_names(., .) %>% 
    map_dfr(function(mode) {
      # ... and gets the Krumhansl-Kessler profile for each one.
      tibble(
        tonic = tonic,
        mode = mode,
        profile = list(get_kk_profile(mode, tonic))
      )
    })
})

kk_all_key_profiles

# Here we define a function that takes a melody as an input
# and returns the estimated key.
kk_estimate_key <- function(
    pitches,  # a vector of pitches for each note in the melody
    durations # a vector of durations for each note in the melody
  ) {
  pc_distribution <- 
    tibble(pitch = pitches, duration = durations) %>%
    mutate(
      # We convert pitches into pitch classes by dividing by 12
      # and taking the remainder.
      pitch_class = factor(pitch %% 12, levels = 0:11)
    ) %>%
    
    # We sum the durations for each pich class.
    group_by(pitch_class, .drop = FALSE) %>%
    summarise(
      duration = sum(duration)
    ) %>%
    
    mutate(
      pitch_class = as.numeric(as.character(pitch_class)),
      # We normalise each duration by dividing by the total duration
      # (this is actually not strictly necessary if we are using
      # standared correlation measures).
      rel_duration = duration / sum(duration)
    )

  correlations <- 
    kk_all_key_profiles %>% 
    mutate(
      correlation = map_dbl(
                  # This function iterates over 
        profile,  # <-- the template profiles
        cor,      # <-- computing the Pearson correlation
        pc_distribution$rel_duration # <-- with the observed durations
      ))
  
  # We choose the profile that maximises the correlation 
  # with the observed distribution.
  chosen <- correlations %>% slice(which.max(correlations$correlation))
  
  chosen %>%
    select(tonic, mode, correlation) %>%
    as.list()
}

# We can use this function to estimate keys for arbitrary melodies,
# where each melody is specified as the combination of
# (a) a list of pitches and 
# (b) a list of durations.
kk_estimate_key(
  pitches =   c(60, 62, 65, 67, 69, 67), 
  durations = c(1, 0.5, 0.5, 1, 0.5, 0.5)
)

kk_estimate_key(
  pitches =   c(60, 63, 65, 67, 68, 67), 
  durations = c(1, 0.5, 0.5, 1, 0.5, 0.5)
)
```

### The Hutchinson-Knopoff dissonance algorithm

#### Overview

The Hutchinson-Knopoff dissonance algorithm attempts to explain how listeners perceive certain chords as dissonant (unpleasant) and others as consonant (pleasant) [@hutchinson1978]. The algorithm is based on the hypothesis that dissonance derives from negatively valenced interactions between the harmonics in a given chord's frequency spectrum. This idea has its roots in the writings of @Helmholtz1875-fm. Though many other consonance/dissonance algorithms have been developed since its original presentation, the Hutchinson-Knopoff algorithm still performs remarkably well when benchmarked against current competitors from the literature [@Harrison2020-gx].

#### Detailed description

The algorithm takes a musical chord as its input. This chord would typically be specified as list of fundamental frequencies for each of its constituent tones; for example, a C dimished triad might be specified by the frequencies 261.6 Hz, 329.6 Hz, and 370.0 Hz.

The first step is to expand each chord tone into its implied harmonics. Each harmonic corresponds to a whole-number multiple of the fundamental frequency, so for example the lowest tone of the triad would contain harmonics at 261.6 Hz, 261.6 \* 2 Hz, 261.6 \* 3 Hz, and so on.

In the absence of further information about the chord, we need to make some assumptions about the amplitudes of the harmonics. In their original paper, Hutchinson and Knopoff propose that the amplitudes could be modelled as following a harmonic series, namely 1, 1/2, 1/3, 1/4, and so on. However, this assumption can easily be replaced with more detailed information about the harmonic amplitudes present in a particular musical timbre.

Expanding the chord tones gives us a frequency spectrum like the following:

```{r, fig.cap = "Frequency spectrum for a closed C diminished triad rooted on middle C (C4). The chord is modelled as containing 10 harmonics per tone, with amplitudes following a harmonic series. The three tones are differentiated by three different colours.", out.width = "100%"}
expand_harmonics <- function(fundamental_frequencies, n_harmonics = 10) {
  if (is.null(names(fundamental_frequencies))) {
    names(fundamental_frequencies) <- seq_along(fundamental_frequencies)
  }
  n_tones <- length(fundamental_frequencies)
  map2_dfr(fundamental_frequencies, names(fundamental_frequencies), function(f0, tone_id) {
    tibble(
      tone_id = tone_id,
      harmonic_number = seq_len(n_harmonics),
      frequency = f0 * harmonic_number,
      amplitude = 1 / harmonic_number
    )
  })
}

c(
  "C4    " = 261.6, 
  "E4    " = 329.6, 
  "F#4    " = 370.0
) %>%
expand_harmonics() %>%
  ggplot(aes(colour = factor(tone_id))) + 
  geom_linerange(aes(x = frequency, ymin = 0, ymax = amplitude)) + 
  scale_x_continuous("Frequency (Hz)") +
  scale_y_continuous("Amplitude") +
  scale_colour_viridis_d("Tones:", end = 0.975) +
  theme(
    legend.direction = "horizontal",
    legend.position = c(0.7, 0.75)
  )
  
```

In the above example, each tone has been expanded into 10 harmonics. Each tone is plotted in a distinct colour, and each harmonic appears as a vertical line with a given frequency and a given amplitude.

The model works by considering pairwise interactions between partials in this spectrum. Each pair of partials contributes something towards the overall dissonance of the chord.

To estimate these pairwise contributions, we iterate over all pairs of partials in the chord. This means first looking at the interaction between partial 1 and partial 2, then at the interaction between partial 1 and partial 3, then at partial 1 and partial 4, and so on. Once we have considered all possible interactions involving partial 1, we then look at the interaction between partial 2 and partial 3, then partial 2 and partial 4, and so on. This process is illustrated in the following animation:

```{r, animation.hook = "gifski", interval = 0.3, out.width = "100%", fig.cap = "Animation illustrating the process of iterating over different pairs of partials (red) in the frequency spectrum."}
spectrum <- c(
    "C4    " = 261.6, 
    "E4    " = 329.6, 
    "F#4    " = 370.0
  ) |>
  expand_harmonics() |> 
  arrange(frequency) |> 
  mutate(partial_number = seq_along(frequency))
n_partials <- nrow(spectrum)
combinations <- combn(n_partials, 2, simplify = FALSE)
for (comb in combinations) {
  p <-
    spectrum |> 
    mutate(
      highlight = seq_along(tone_id) %in% comb
    ) |> 
    ggplot(aes(colour = factor(highlight, levels = c("TRUE", "FALSE")))) + 
    geom_linerange(aes(x = frequency, ymin = 0, ymax = amplitude)) + 
    scale_x_continuous("Frequency (Hz)") +
    scale_y_continuous("Amplitude") +
    scale_colour_manual(values = c("red", "black")) +
    ggtitle(
      sprintf("Partials: %s, %s", comb[[1]], comb[[2]])
    ) +
    theme(
      legend.position = "none"
    )
  print(p)
}
```

Let's suppose we are trying to calculate the dissonance induced by partials 4 and 5. We begin by writing down their frequencies and amplitudes:

```{r}
spectrum |> 
  slice(c(4, 5)) |> 
  select(
    Partial = partial_number,
    "Frequency (Hz)" = frequency,
    Amplitude = amplitude
  ) |> 
  knitr::kable()
```

The elicited dissonance comes from interference between these two partials. The extent to which two partials interfere depends on the *critical bandwidth* for those partials. The critical bandwidth tells us how close two partials have to be before they start interfering with each other. It is typically estimated using psychoacoustic experiments measuring masking effects.[^computational-music-psychology-2]

[^computational-music-psychology-2]: Masking is when a particular sound obscures the perception of another sound. It is analogous to the phenomenon of occlusion in vision perception.

The critical bandwidth is thought to correspond to a certain physical distance on the basilar membrane, estimated to be approximately 1 mm. Tones within a certain physical distance cause overlapping excitation patterns, producing interference effects such as masking and beating.

```{r, include = FALSE}
frequency <- 200
cbw <- 1.72 * (frequency ^ 0.65)

frequency <- 10000
cbw <- 1.72 * (frequency ^ 0.65)
```

For low frequencies, a given physical distance on the basilar membrane will correspond to a relatively small frequency difference. For example, at 200 Hz, a 1 mm movement corresponds to a frequency change of approximately 50 Hz; in contrast, at 10,000 Hz, a 1 mm movement corresponds to a frequency change of approximately 700 Hz.

Since critical bandwidth depends on physical distance, and since the same physical distance corresponds to higher frequency distances at higher frequencies, we find that critical bandwidth also increases for higher frequencies. @hutchinson1978 model this relationship using the following mathematical expression:

```{=tex}
\begin{equation}
\mathrm{critical\ bandwidth}=1.72 \times \mathrm{frequency}^{0.65}
(\#eq:critical-bandwidth)
\end{equation}
```
where both critical bandwidth and frequency are expressed in Hz. This relationship looks like the following:

```{r critical-bandwidth-frequency, fig.cap = "Critical bandwidth as a function of frequency."}
df_cbw <- 
  tibble(
    frequency = seq(from = 20, to = 20000),
    midi = 69 + 12 * log2(frequency / 440),
    log_frequency = log(frequency),
    cbw_dist_hz = 1.72 * (frequency ^ 0.65),
    cbw_dist_freq_ratio = (frequency + cbw_dist_hz / 2) / (frequency - cbw_dist_hz / 2),
    cbw_dist_semitones = 12 * log2(cbw_dist_freq_ratio)
  )

df_cbw %>%
  ggplot(aes(frequency, cbw_dist_hz)) + 
  geom_line() + 
  scale_x_continuous("Frequency (Hz)") + 
  scale_y_continuous("Critical bandwidth (Hz)")
```

It is interesting to express the same relationship using pitch notation instead of frequency notation. If we do so, we get the following curve:

```{r, fig.cap = "Critical bandwidth as a function of pitch.", out.width = "100%"}
df_cbw %>%
  filter(frequency < 7000) %>%
  ggplot(aes(midi, cbw_dist_semitones)) + 
  geom_line() + 
  scale_x_continuous(
    "Pitch",
    breaks = c(24, 36, 48, 60, 72, 84, 96),
    labels = c("C1", "C2", "C3", "C4", "C5", "C6", "C7")
  ) + 
  scale_y_continuous(
    "Critical bandwidth (semitones)"
  )
```

The relationship appears to be inverted: we see now that, as pitch height increases, the critical band corresponds to *narrower* pitch intervals (as measured in semitones). Why is this? Well, before we were measuring critical bandwidth in terms of absolute frequency differences (measured in Hz), whereas now we are measuring critical bandwidth in terms of pitch intervals, which correspond to frequency *ratios*. While the absolute frequency distance does indeed increase as the base frequency increases, it doesn't decrease as fast as the base frequency. This means that the frequency ratio increases, and hence the pitch interval increases.

This phenomenon has an interesting consequence for dissonance perception. It implies that a particular pitch interval may sound dissonant when played in a lower register, on account of the tones being localised to the same critical band, but may then become consonant when played in a higher register, because the critical bandwidth decreases causing the tones to be localised to separate critical bands. This phenomenon seems to provide a good explanation for voice spacing patterns in common-practice Western tonal music [@huron1992; @harrison2020].

Let us return to the task of modelling the dissonance induced by partials 4 and 5. We begin by estimating the local **critical bandwidth** for these two partials. We do this by taking the average of the two frequencies and feeding it into Equation \@ref(eq:critical-bandwidth) (Figure \@ref(fig:critical-bandwidth-frequency)). In our example, we had frequencies of 532.2 Hz and 659.2 Hz, so our mean frequency is 591.2 Hz. So, our estimated critical bandwidth is

$$
1.72 \times 591.2 ^ {0.65} = 108.9 \mathrm{\ Hz}.
$$

Note that instead of using the mathematical equation, we could alternatively have found this number graphically, by taking a zoomed-in version of Figure \@ref(fig:critical-bandwidth-frequency), drawing a vertical line upwards from 591.2 Hz on the horizontal axis, finding its intersection with the plot line, and drawing a horizontal line to the vertical axis:

```{r, fig.cap = "Calculating critical bandwidth from pitch.", out.width = "100%", animation.hook = "gifski", interval = 0.9}
p <- 
  df_cbw |> 
  filter(frequency <= 800) |> 
  ggplot(aes(frequency, cbw_dist_hz)) + 
  geom_line() + 
  scale_x_continuous("Frequency (Hz)") + 
  scale_y_continuous("Critical bandwidth (Hz)", breaks = seq(from = 0, to = 150, by = 25))

print(p)

p <- p + 
  geom_vline(xintercept = 591.2, colour = "red", linetype = "dashed") + 
  geom_point(aes(x, y), data = tibble(x = 591.2, y = 108.9), colour = "red")
  
print(p)

p <- p + 
  geom_hline(yintercept = 108.9, colour = "red", linetype = "dotted")

print(p)
print(p)
print(p)
```

We then express the distance between these two partials in units of critical bandwidth. We achieve this simply by dividing the distance in Hz by the critical bandwidth. This gives us:

$$
\frac{659.2 - 532.2}{108.9} = 1.17
$$

We then plug this number (1.17) into an idealised *dissonance curve* that characterises how dissonance varies as a function of the critical bandwidth distance between a pair of pure tones. Hutchinson & Knopoff's dissonance curve looks something like an inverted U, with a peak at 0.25 critical bandwidths:[^computational-music-psychology-3]

[^computational-music-psychology-3]: Hutchinson & Knopoff presented their original curve graphically as an interpolation of results from previous psychoacoustic experiments. Bigand and colleagues (1996) subsequently introduced a parametric approximation that takes the following form:

    $$
    \left(
    4y \exp \left( 1 - 4y \right) 
    \right) ^ 2
    $$

    We tend to prefer the latter form because it is easy to implement computationally.

```{r, fig.cap = "Idealised dissonance curve from @hutchinson1978, plotted using the parametric version specified by @Bigand1996.", animation.hook = "gifski", interval = 0.9, out.width = "100%"}
if (FALSE) {
  # Verifying the maximum is at 0.25
  optim(0.5, dycon::hutch_g, control = list(fnscale = -1))
}

p <- 
  tibble(
    cbw_dist = seq(from = 0, to = 2, by = 0.01),
    dissonance = dycon::hutch_g(cbw_dist, cbw_cut_off = 1e6)
  ) %>%
  ggplot(aes(cbw_dist, dissonance)) + 
  geom_line() + 
  scale_x_continuous("Distance between partials (in critical bandwidths)") + 
  scale_y_continuous("Dissonance factor")

print(p)

p <- p + 
  geom_vline(xintercept = 1.17, colour = "red", linetype = "dashed") + 
  geom_point(aes(x, y), data = tibble(x = 1.17, y = 0.0139), colour = "red")
  
print(p)

p <- p + 
  geom_hline(yintercept = 0.0139, colour = "red", linetype = "dotted")

print(p)
print(p)
print(p)
```

We find that, since the two partials are separated by more than one critical bandwidth, the resulting dissonance factor is rather low (just 0.014).

The final step is to account for amplitude. We do two things here. First, we compute the *product of the amplitudes* for the two partials currently under consideration. Since both partials had amplitudes of 0.5, we get:

$$
0.5 \times 0.5 = 0.25.
$$

Second, we compute the *sum of squared amplitudes* over all the partials in the spectrum. Squaring means multiplying a number by itself; for example, two squared (written $2^2$) is four, three squared (written $3^2$) is nine, and so on. We compute the sum of squared amplitudes by first squaring all the amplitudes in the spectrum:

```{r}
spectrum |> 
  mutate(
    squared_amplitude = amplitude ^ 2,
    frequency = sprintf("%.1f", frequency),
    amplitude = sprintf("%.2f", amplitude),
    squared_amplitude = sprintf("%.2f", squared_amplitude)
  ) |> 
  select(
    Frequency = frequency, 
    Amplitude = amplitude,
    "Squared amplitude" = squared_amplitude
  ) |> 
  DT::datatable(
  options = list(
    searching = FALSE,
    rowId = FALSE
  ))
```

then taking their sum:

$$
1.00 + 1.00 + 1.00 + 0.25 + 0.25 + 0.25 + \ldots = `r sum(spectrum$amplitude ^ 2) |> round(3)`.
$$

Note that this expression is the same no matter which pair of partials we are considering, so we only need to compute it once for the whole spectrum.

To get the final dissonance contribution of the two partials, we then take the previously computed dissonance factor (0.014), multiply it by the product of the amplitudes (0.25), and divide it by the sum of the squared amplitudes (4.469). This gives us:

$$
\frac{0.014 \times 0.25}{4.469} = 0.0008 
$$

Let's think about what role amplitude plays in this procedure. The fact that we multiply by the product of the amplitudes means that the dissonance contribution is *proportional* to the amplitudes of each of the two partials. In other words, if we double the amplitude of either partial, the resulting dissonance contribution will be doubled. Dividing by the sum of the squared amplitudes, meanwhile, has the effect of normalising by the spectrum's overall amplitude. The consequence in particular is that dissonance ends up being independent of the chord's *overall* amplitude; for example, if we double the amplitude of every partial in the spectrum, then the overall dissonance will stay the same. It is only by changing the amplitudes of some partials and not others that the overall dissonance will change.

We repeat this process for every pair of partials in the spectrum. This gives us a table like the following:

```{r}
tibble(
  i1 = map_int(combinations, 1),
  i2 = map_int(combinations, 2),
  f1 = spectrum$frequency[i1],
  f2 = spectrum$frequency[i2],
  a1 = spectrum$amplitude[i1],
  a2 = spectrum$amplitude[i2],
  f_mean = (f1 + f2) / 2,
  cbw = dycon::hutch_cbw(f1, f2),
  cbw_dist = abs(f2 - f1) / cbw,
  dissonance_factor = dycon::hutch_dissonance_function(f1, f2),
  a1_a2 = a1 * a2,
  sum_sq_amp = sum(spectrum$amplitude ^ 2),
  dissonance = dissonance_factor * a1_a2 / sum_sq_amp
) |> 
  mutate(
    across(where(is.double), ~ sprintf("%.2f", .))
  ) |> 
  select(- i1, -i2) |> 
  rename(
    "Frequency 1" = f1,
    "Frequency 2" = f2,
    "Amplitude 1" = a1,
    "Amplitude 2" = a2,
    "Mean frequency" = f_mean,
    "Critical bandwidth" = cbw,
    "Critical bandwidth distance" = cbw_dist,
    "Dissonance factor" = dissonance_factor,
    "Product of amplitudes" = a1_a2, 
    "Sum of squared amplitudes" = sum_sq_amp,
    "Dissonance" = dissonance
  ) |> 
  DT::datatable(
  options = list(
    searching = FALSE,
    rowId = FALSE,
    columnDefs = list(list(className = 'dt-center', targets = "_all"))
  ))
```

, on account of interference within the critical band, but will become consonant when played in a sufficiently high register, because

a given frequency difference in Hz corresponds to

The model supposes that the dissonance induced by a single pair of partials depends on two primary factors: *distance* and *amplitude*.

The amplitude effect works as follows: dissonance is modelled as being *proportional* to the amplitudes of both partials. This means that doubling the amplitude of a particular partial will double the dissonance contribution of that partial. There is additionally a normalising factor which we will describe later.

The distance effect is a modelled as a smooth function that looks something like an inverted U:

Notice the units of the horizontal axis: *critical bandwidths*. Critical bandwidths are closely related to the notion of physical distance within the cochlea. Tones that are separated by less than one critical bandwidth are close enough to interfere physically one another through beating and masking effects. According to this model, peak dissonance occurs when the tones are separated by 0.25 critical bandwidths.

Critical bandwidth varies as a function of pitch height. The following figure plots critical bandwidth in semitones as a function of pitch expressed using letter names (C1, C2, C3), where numbers denote octaves. We can see that, as pitch decreases, critical bandwidth increases. As a consequence, intervals that elicit little dissonance in high registers will end up eliciting high dissonance in low registers.

These factors of amplitude and distance are combined in the following mathematical expression:

$$
d_{ij}= \frac{A_iA_jg_{ij}}{\sum_k{A_k^2}}
$$

where:

-   $d_{ij}$ is the dissonance contribution from the interactions between partial $i$ and partial $j$

-   $A_i$ is the amplitude of partial $i$

-   $A_j$ is the amplitude of partial $j$

-   $g_{ij}$ is the dissonance component that depends on the critical bandwidth distance between partials

-   $\sum_k{A_k^2}$ is a normalising factor, corresponding to the sum of the squared amplitudes of all of the partials. This normalising factor ensures that dissonance is independent of the overall amplitude of the chord as a whole.

This mathematical notation may be a little unfamiliar to some of you. There are a couple of mathematical notations to note here. The first is *index notation*, where we write small letters or numbers in subscripts, for example:

$$
A_i
$$

The subscript expression ($i$ in the above) is called the index. It is used as a counter to differentiate between multiple versions of an object. For example, in our above model, $A_1$ denotes the amplitude of the first partial, $A_2$ denotes the amplitude of the second partial, and so on.

When we write mathematical expressions using indices, we often write them in *generic* forms. This means that, instead of writing $A_1$ or $A_2$, we write $A_i$, which refers to the $i$th partial.

So, to calculate the dissonance contribution of a pair of pure tones, we must first calculate the critical bandwidth at their mean frequency[^computational-music-psychology-4], then plug this into the dissonance curve. Suppose for e

[^computational-music-psychology-4]: This is the way Hutchinson and Knopoff do it (calculate the mean frequency, then plug it into the critical bandwidth plot); there are however more principled ways that one could do this.

However, we must also take into account the amplitudes of the tones. Hutchinson and Knopoff claim in particular that the dissonance contribution is proportional to the *product* of the two tones' amplitudes [TODO divided by the sum of the ; or just normalising factor?]; in other words, if we double the amplitude of any one of these tones, the overall dissonance contribution will be doubled.

So, we have a procedure for calculating the dissonance contribution of a given pair of pure tones.

The model breaks down the aggregate dissonance of the chord into many

The model supposes that dissonance is an *additive* phenomenon, whereby the aggregate dissonance of the chord comes from adding together the induced

## What role does modelling play in the scientific method?

## Modelling methodologies

Computer models are generally used for one of the three following goals: theory development, theory evaluation, and prediction generation.

### Theory development

Computer models can help us by filling in missing gaps of existing theories.

One useful application of computer models is for filling in missing gaps of theories. For example,

Suppose that we have developed a promising but incomplete theory of

Suppose that we have decided to study a particular psychological phenomenon, and suppose that we have a hypothetical

that we have a psychological phenomenon that we wish to understand, an

have identified a psychological phenomenon that we wish to understand, and we have an incomplete

This is the part of the research pipeline where we have identified a particular psychological phenomenon that we wish to study

-   we write down a model that has some missing bits

-   we use data to fill in the missing bits

    -   regression is one way of doing that

    -   parameter optimisation another (this is a generalisation of regression)

In theory development, we have identified a particular interesting psychological phenomenon, and we are trying to development

-   Optimising a memory decay hyperparameter

-   Optimising the weights of different features in an emotion model (regression)

    -   Is a regression model really a computational model? yes....?

        -   methods for developing models:

            -   rational analysis

            -   regression

-   Comparing roughness and harmonicity models

### Theory evaluation

### Theory development

to support one of three research tasks: theory development, theory evaluation, and prediction

fits into one of three stages of the research pipeline: theory development, theory evaluation, or prediction generation.

-   Theory development

-   Theory evaluation

-   Prediction generation

Computer models have two main applications in psychological research. When the problem domain is not yet well understood, computer models can play an important role in *theory development*, helping researchers to choose between competing explanations of a phenomenon. Once the problem domain becomes well-understood, computer models then become valuable for *prediction generation*, allowing the researcher to predict the outcome of an experiment without actually running it.

### Computer models for theory development

We begin this process by identifying an interesting psychological phenomenon which we wish to study. For example, we might be interested in studying listeners' perception of emotion in melodies.

The next step is to identify a set of candidate psychological mechanisms for the phenomenon. These candidate mechanisms will often be derived from previous literature, but they may also be derived from our own intuitions. These candidate mechanisms may or may not be mutually exclusive. In the case of melodic emotion perception, we might hypothesise multiple non-exclusive mechanisms, including the recognition of ascending versus descending contours, major versus minor modes, legato versus staccato articulation, and so on.

We then proceed by implementing computer models that simulate these mechanisms. Each computer model will typically be a piece of computer software that takes a musical stimulus as an input and simulates the psychological processing of that stimulus. The output will typically be one or more numbers that capture the outcome of the process. For example, a simple mode recognition algorithm might return 1 for major-key melodies and 0 for minor-key melodies.

In the next step, we design an experiment for probing these models.

For example, we might decide to study *key finding*, the process by which a listener identifies the key of a g

1.  **Identify an interesting psychological phenomenon.** For example, we might be interested in studying listeners' perception of emotion in melodies.

2.  **Identify candidate psychological mechanisms for this phenomenon.** These candidate mechanisms will often be derived from previous literature, but they may also be derived from our own intuitions. These candidate mechanisms may or may not be mutually exclusive. In the case of melodic emotion perception, we might hypothesise multiple non-exclusive mechanisms, including the recognition of ascending versus descending contours, major versus minor modes, legato versus staccato articulation, and so on.

3.  **Implement computer models that simulate these mechanisms.** Each computer model will typically be a piece of computer software that takes a musical stimulus as an input and simulates the psychological processing of that stimulus. The output will typically be one or more numbers that capture the outcome of the process. For example, a simple mode recognition algorithm might return 1 for major-key melodies and 0 for minor-key melodies.

4.  **Design an experiment for probing these models.** The experiment could simply comprise a set of musical stimuli to be presented to the participant.

5.  **Generate model predictions.** This will typically involve running the models on each stimulus and recording the models' outputs.

6.  This stimulus set should be designed to systematically probe the different computer models.

Suppose we have identified a problem domain which we want to study, for example *key finding*, the process by which a listener identifies the key of a given musical passage.

### Computer models for prediction generation

complem

Second, they can be used for *prediction generation*

When the problem domain is still not well-understood, models can be used as a tool for *theory*

## Case studies

```{r}

```

### Hutchinson & Knopoff's dissonance model

### Other models

## Modelling traditions

## Modelling and empirical research

The basic methodology of computational music psychology works as follows:

1.  **Identify an interesting psychological phenomenon.** For example, we might be interested in studying listeners' perception of emotion in melodies.

2.  **Identify candidate psychological mechanisms for this phenomenon.** These candidate mechanisms will often be derived from previous literature, but they may also be derived from our own intuitions. These candidate mechanisms may or may not be mutually exclusive. In the case of melodic emotion perception, we might hypothesise multiple non-exclusive mechanisms, including the recognition of ascending versus descending contours, major versus minor modes, legato versus staccato articulation, and so on.

3.  **Implement computer models that simulate these mechanisms.** Each computer model will typically be a piece of computer software that takes a musical stimulus as an input and simulates the psychological processing of that stimulus. The output will typically be one or more numbers that capture the outcome of the process. For example, a simple mode recognition algorithm might return 1 for major-key melodies and 0 for minor-key melodies.

4.  **Design an experiment for probing these models.** The experiment could simply comprise a set of musical stimuli to be presented to the participant.

5.  **Generate model predictions.** This will typically involve running the models on each stimulus and recording the models' outputs.

6.  This stimulus set should be designed to systematically probe the different computer models.

The basic paradigm of computational music psychology works as follows:

1.  Identify a particular psychological phenomenon of interest

2.  Compile or develop one or more candidate psychological theories for explaining that phenomenon

3.  Implement these theories as computational models.

4.  Design one or more experimental paradigms for probing these models.

5.  Generate model predictions for these experimental paradigms. If there are multiple models, these experimental paradigms should ideally yield dissociated model predictions. If not, maybe revisit the design.

6.  Compile empirical datasets from human listeners for these experimental paradigms.

7.  Compare the human responses to the model predictions.

    1.  Can compare models

    2.  Can optimise model parameters

Note: sometimes the datasets exist already.

Computational music psychologists work by developing and evaluating computer simulations of mental processes underlying musical behaviours such as music listening, music performance, and music composition. These simulations take as input ... and return as output...

The goal of ordinary music psychology research is to develop empirically grounded theories of the mental processes underlying musical behaviours such as music listening, music performance, and music composition. Typical methods include both behavioural experiments, where participants are instructed to make judgments in response to particular musical stimuli, and neuroscientific experiments, where participants have their brain responses recorded while listening to various musical stimuli.

Computational music psychology incorporates an additional component of computer simulation into this paradigm.

aim to develop effective computer simulations of the mental process

The goal of music psychology research is to understand the mental processes underlying musical behaviours such as music listening, music performance, and music composition. $$...$$ $$Typical methods include$$

Computational music psychology is a branch of music psychology that incorporates computer simulations into its workflow. These simulations are intended to represent the biological and psychological processes underpinning a particular

research

has similar goals, but incorporates

What is music psychology?

What is the role of the computer here?

Why would you use a computer here?

### Motivations

As discussed already

### Computational metaphor (information processing)

Introduce idea of data structures and algorithms

Relate to cognitive equivalents

### Traditions

-   Symbolic versus connectionist AI

-   Cognitive versus neuro

-   Psychoacoustic

### Audio vs symbolic processing

### Strategies

Intuitive:

-   Just write down what we think happens

Marr:

-   What problem is the mind trying to solve?

-   What algorithm is used to implement that solution?

-   How is that algorithm implemented in the brain?

Rational observer:

-   Derive the optimal solution to a task

-   Compare this to the brain

Bounded rationality:

-   Like rational observer approach, but incorporating processing constraints

Learning by imitation:

-   Machine learning
