# Computational music psychology

```{r, include = FALSE}
library(tidyverse)
library(ggpubr)
theme_set(theme_pubr())
source("setup.R")
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)

```

Computational music psychology is a kind of music psychology that relies heavily on computer models. These computer models are special algorithms that simulate certain acoustic, biological, and/or mental processes thought to be involved in particular musical behaviours. Used correctly, they can form an integral part of the scientific process.

## Why use computer models?

Computer models can bring several important benefits to music psychology:

**Addressing woolliness.** When we write about psychological theories with words, it's easy for ambiguities to slip in, making our arguments 'woolly'. In contrast, if we implement our theory as a computational model, we are forced to be completely explicit about what we mean. In the process, we may uncover important unwritten assumptions or logical gaps in our theory that deserve addressing.

**Enhancing testability.** A good scientific theory should be good at predicting observed phenomena. In order to test this, we must generate predictions from the theory in various empirical scenarios. When our theory is only specified verbally, it can be difficult to work out just what a theory predicts in a given scenario; conversely, once we've collected empirical data, it can be difficult to work out exactly what these data mean for the theory. Computational modelling addresses these problems. The model directly generates numeric predictions for different empirical scenarios, which can then be related to empirical data using standard statistical methods. In music psychology research, this feature is especially valuable for helping us to run psychological experiments using realistic music rather than basic artificial stimuli.

**Creating useful tools.** Suppose we end up producing a computational implementation of a particular cognitive model. The original motivation of producing this computational model may have been to obtain testable predictions for upcoming behavioural experiments. However, as a byproduct we have obtained a piece of software that is capable of solving the particular cognitive task that we are studying. Depending on the cognitive task and the success of the implementation, this can be a useful outcome in itself. For example, suppose that we have implemented a cognitive model for recognising emotion in melodies; we could then use a software implementation of this model for applications such as automated playlist generation.

## What do music psychology models look like?

The literature contains many different computational models of music perception and production (see [@Temperley2013-fg] for a review). We will begin by introducing a couple of particularly prominent such models: the Krumhansl-Schmuckler key-finding algorithm [@krumhansl1990cognitive], and the Hutchinson-Knopoff dissonance algorithm [@hutchinson1978].

### The Krumhansl-Schmuckler key-finding algorithm

#### Overview

This algorithm is designed to simulate how listeners identify the *key* of a given musical passage. Now, identifying keys is often trivial to someone reading a musical score. Many musical pieces have their key written in their title, for example "Sonata No. 6 in D major, K. 284" or "Symphony No. 9 in D minor, Op. 125". Even if the key is not present in the title, we can typically narrow down the key to one of two options by inspecting the key signature at the beginning of the piece. However, none of this written information is necessarily available to the music *listener*; they must instead somehow identify the key purely from the notes that they hear.

The Krumhansl-Schmuckler algorithm proposes a mechanism for this process. The algorithm can be succinctly summarised as follows:

1.  Different keys use different pitch classes to different degrees.[^computational-music-psychology-1]

2.  Listeners possess internal templates that summarise the pitch-class distribution for different musical keys.

3.  During a musical piece, listeners track the distribution of pitch classes within that piece. The inferred key then corresponds to the key whose pitch-class distribution matches best to the observed distribution.

[^computational-music-psychology-1]: Pitch classes denote sets of pitches separated by whole numbers of octaves. In the Western 12-tone scale, there are exactly 12 pitch classes: C, C\#, D, D\#, E, F, F\#, G, G\#, A, A\#, and B.

#### Implementation details

In order to implement this as a computer model, we need to fill in a few details.

First, we ask, what exactly do the listeners' templates look like? In the original Krumhansl-Schmuckler algorithm, these templates are derived empirically from behavioural experiments using Krumhansl and Kessler's [@krumhansl1982] *probe-tone* paradigm, where the listener is asked to rate the 'fit' of a particular musical note in a particular musical context. These experiments produced the following templates:

```{r}
kk_profiles <- list(
  # This list contains two profiles: one for the major mode and one for the minor mode.
  Major = c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88),
  Minor = c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
)
```

```{r, echo = FALSE, out.width = "100%"}
kk_profiles %>% 
  as_tibble() %>% 
  mutate(chromatic_scale_degree = 0:11) %>% 
  pivot_longer(cols = c("Major", "Minor"), names_to = "mode", values_to = "rating") %>% 
  ggplot(aes(chromatic_scale_degree, rating, colour = mode)) + 
  geom_line() + 
  scale_x_continuous("Interval above the tonic (semitones)", breaks = 0:12) + 
  scale_y_continuous("Fit") +
  scale_colour_manual(values = c("dodgerblue3", "darkgoldenrod1")) +
  facet_wrap(~ mode, ncol = 1) +
  theme(
    legend.position = "none",
    panel.grid.major.x = element_line(colour = "grey87"),
    strip.background = element_blank(),
    strip.text = element_text(face = "bold"),
    panel.spacing = unit(3, "lines")
  )
```

In the above figure, pitch classes are expressed as intervals above the tonic, written in semitones. This means for example that the tonic corresponds to 0 semitones, the major third corresponds to 4 semitones, and the perfect fifth to 7 semitones.

Note how the major template exhibits high fit for pitch classes from the major scale (0, 2, 4, 5, 7, 9, 11), and low fit for the other pitch classes (1, 3, 6, 8, 10). Similarly, the minor template exhibits high fit for pitch classes from the minor scale (0, 2, 3, 5, 7, 8) and low fit for the others.

To derive the pitch-class template for a given musical key, we need to replot the horizontal axis in terms of absolute pitch classes rather than intervals above the tonic. To achieve this transformation, we simply take the interval and add it onto the tonic. For example, if we are in F major, then an interval of 0 semitones corresponds to F, an interval of 1 semitone corresponds to F\#, and so on.

The following figure illustrates the resulting pitch-class profiles for different tonics, with the tonic marked as a dashed red line. As the tonic moves upwards, the pitch-class profile 'rotates' along with it.

```{r, echo = FALSE, out.width = "100%", animation.hook = "gifski"}
for (tonic in 0:11) {
  pcs <- c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B")
  p <- 
    kk_profiles %>% 
    as_tibble() %>% 
    mutate(chromatic_scale_degree = 0:11) %>% 
    pivot_longer(cols = c("Major", "Minor"), names_to = "mode", values_to = "rating") %>%
    mutate(
      pitch_class = (tonic + chromatic_scale_degree) %% 12
    ) %>%
    ggplot(aes(pitch_class, rating, colour = mode)) + 
    geom_vline(xintercept = tonic, colour = "red", linetype = "dashed") +
    geom_line() + 
    scale_x_continuous(
      "Pitch class", 
      breaks = 0:12,
      sec.axis = sec_axis(
        trans = identity,
        breaks = 0:11,
        labels = pcs
      )
    ) + 
    scale_y_continuous("Fit") +
    scale_colour_manual(values = c("dodgerblue3", "darkgoldenrod1")) +
    facet_wrap(~ mode, ncol = 1) +
    theme(
      legend.position = "none",
      panel.grid.major.x = element_line(colour = "grey87"),
      strip.background = element_blank(),
      strip.text = element_text(face = "bold"),
      panel.spacing = unit(3, "lines"),
      title = element_text(face = "bold")
    ) + 
    ggtitle(paste0("Tonic: ", pcs[1 + tonic]))
  print(p)
}
```

So, we have now established what the templates look like for the 24 major and minor keys. There remains one question, however: how do we compare the listener's pitch-class distribution to these template distributions? There are various possibilities here [@albrecht2013], but the original Krumhansl-Kessler algorithm uses a standard statistic called the *Pearson correlation*. We don't need to worry about the mathematical details here. The important thing to know is that the Pearson correlation can be interpreted as a measure of *similarity*, and it takes values between -1 (maximally dissimilar) and 1 (maximally similar).

#### Worked example

Let's now see this algorithm applied in practice. We'll take the well-known lullaby 'Twinkle, Twinkle, Little Star', which looks like the following in traditional music notation:

![Twinkle, Twinkle, Little Star. Credit: Helix84, [CC BY-SA 3.0](http://creativecommons.org/licenses/by-sa/3.0/), via Wikimedia Commons.](images/twinkle-twinkle.png){width="100%"}

Our first step is to translate this melody into machine-readable notation. We can write it in tabular format, where the first column is the pitch (expressed as a MIDI note), and the second column is the duration (expressed in crotchets, or quarter notes).

```{r}
twinkle <- tribble(
  ~ pitch,  ~ duration,
  60, 1,
  60, 1,
  67, 1,
  67, 1,
  69, 1,
  69, 1,
  67, 2,
  
  65, 1,
  65, 1,
  64, 1, 
  64, 1,
  62, 1,
  62, 1, 
  60, 2,
  
  67, 1, 
  67, 1,
  65, 1,
  65, 1,
  64, 1, 
  64, 1,
  62, 2,
  
  67, 1, 
  67, 1,
  65, 1,
  65, 1,
  64, 1, 
  64, 1,
  62, 2,
  
  60, 1,
  60, 1,
  67, 1,
  67, 1,
  69, 1,
  69, 1,
  67, 2,
  
  65, 1,
  65, 1,
  64, 1, 
  64, 1,
  62, 1,
  62, 1, 
  60, 2,
)
twinkle %>%
  set_names(c("Pitch", "Duration")) %>%
  DT::datatable(
    options = list(
      searching = FALSE,
      rowId = FALSE
    ))
```

We then compute the relative durations of each pitch class in the melody. To convert pitches to pitch classes we divide by 12 and take the remainder; for example, the pitch 67 is equal to 12 \* 5 + 7, so its pitch class is 7. Applying this logic, and expressing durations as a percentage of the total melody duration, we get the following:

```{r, out.width = "100%"}
twinkle_profile <- 
  twinkle %>%
  mutate(
    pitch_class = factor(pitch %% 12, levels = 0:11)
  ) %>%
  group_by(pitch_class, .drop = FALSE) %>%
  summarise(
    duration = sum(duration)
  ) %>%
  mutate(
    pitch_class = as.numeric(as.character(pitch_class)),
    rel_duration = duration / sum(duration)
  )

twinkle_profile %>%
  ggplot(aes(pitch_class, 100 * rel_duration)) + 
    geom_line(colour = "dodgerblue3") + 
    scale_x_continuous(
      "Pitch class", 
      breaks = 0:12,
      sec.axis = sec_axis(
        trans = identity,
        breaks = 0:11,
        labels = pcs
      )
    ) + 
  scale_y_continuous("Relative duration (%)")
```

Now our task is to compare this profile with all the different reference profiles, to work out which provides the best fit. The following animation evaluates each reference profile in turn (top panel) and displays the resulting correlation (bottom panel).

```{r, out.width = "100%", animation.hook = "gifski", interval = 1.5, fig.height = 7}
get_kk_profile <- function(mode, tonic) {
  ref_profile <- kk_profiles[[mode]]
  absolute_pitch_classes <- 0:11
  relative_pitch_classes <- (absolute_pitch_classes - tonic) %% 12
  ref_profile[1 + relative_pitch_classes]
}

kk_all_key_profiles <- map_dfr(0:11, function(tonic) {
  c("Major", "Minor") %>% 
    set_names(., .) %>% 
    map_dfr(function(mode) {
      tibble(
        tonic = tonic,
        mode = mode,
        profile = list(get_kk_profile(mode, tonic))
      )
    })
}) %>%
  arrange(mode)

kk_correlations <- 
    kk_all_key_profiles %>% 
    mutate(
      correlation = map_dbl(
        profile,
        cor, 
        twinkle_profile$rel_duration
      ))

.tmp <- pmap(kk_all_key_profiles, function(tonic, mode, profile) {
  tonic_label <- pcs[tonic + 1]
  key_label <- paste0(tonic_label, " ", mode)
  
  plot_profile <- 
    bind_rows(
      twinkle_profile |> 
        select(pitch_class, rel_duration) |> 
        mutate(group = "Twinkle twinkle"),
      tibble(
        pitch_class = 0:11,
        rel_duration = profile / sum(profile),
        group = key_label
      )
    ) |> 
    mutate(
      group = factor(group, levels = c("Twinkle twinkle", key_label))
    ) |> 
    ggplot(aes(pitch_class, rel_duration * 100, colour = group)) +
    geom_vline(xintercept = tonic, colour = "red", linetype = "dashed") +
    geom_line() + 
    scale_x_continuous(
      "Pitch class", 
      breaks = 0:12,
      sec.axis = sec_axis(
        trans = identity,
        breaks = 0:11,
        labels = pcs
      )
    ) + 
    scale_y_continuous("Relative duration (%)") +
    scale_colour_manual(NULL, values = c("dodgerblue3", "darkgoldenrod1")) +
    ggtitle(
      paste0("Key: ", key_label)
    ) +
    theme(
      legend.position = "right",
      plot.title = element_text(
        face = "bold"
      )
    )
  
  plot_all_correlations <- 
    kk_correlations |> 
    mutate(selected = (tonic == !!tonic & mode == !!mode)) |> 
    ggplot(aes(tonic, correlation,
               colour = mode, 
               shape = selected,
               size = selected)) + 
    geom_point() + 
    scale_x_continuous(
      "Tonic", 
      breaks = 0:12,
      sec.axis = sec_axis(
        trans = identity,
        breaks = 0:11,
        labels = pcs
      )
    ) +
    scale_y_continuous("Pearson correlation", limits = c(-1, 1)) +
    scale_shape_manual(values = c(1, 16), guide = "none") + 
    scale_size_manual(values = c(2, 5), guide = "none") + 
    scale_colour_manual(NULL, values = c("dodgerblue3", "darkgoldenrod1")) + 
    theme(
      legend.position = "right"
    )
  egg::ggarrange(
    plot_profile,
    plot_all_correlations,
    ncol = 1
  ) |> 
    print()
})

```

The results can be expressed in a table like the following. You can click on particular column headers to sort by the values in that column.

```{r}
kk_correlations |> 
  mutate(
    tonic = plyr::mapvalues(tonic, from = 0:11, to = pcs),
    correlation = sprintf("%.2f", correlation)
  ) |> 
  select(
    Tonic = tonic,
    Mode = mode,
    Correlation = correlation
  ) |> 
  DT::datatable(
    options = list(
      searching = FALSE
    )
  )
```

If we sort the 'Correlation' column by descending values, we see the following top three key candidates: C major, F major, and G major. Conversely, if we sort the column by ascending values, we see the following bottom three candidates: F\# major, A minor, and C\# major. So, the algorithm selects C major as the inferred key for the melody, which happily is consistent with what most music theorists would say if they examined the score.

#### Where should the templates come from?

The original Krumhansl-Schmuckler algorithm uses template profiles derived from behavioural experiments using the probe-tone paradigm. The implementation leaves open the question of where exactly these templates come from.

One possibility is that the templates are learned through exposure to particular musical styles. By hearing pieces in particular musical keys, the listener learns that particular keys tend to be associated with particular pitch-class distributions. This learning process is complicated by the fact that the listener has to learn the key profiles without being told what key individual pieces are in. Inspiration for understanding this process can come from the machine-learning field of *unsupervised learning*, which studies algorithms for learning structure from unlabelled data. In particular, several key-finding algorithms have used a technique called *latent Dirichlet allocation* to learn both key labels and key profiles directly from unlabelled musical corpora [@moss2021; @Hu2009].

#### Limitations

The Krumhansl-Schmuckler algorithm has been very influential within the field of music psychology, and many subsequent key-finding algorithms use a similar kind of template-matching process [@albrecht2013; @Bellmann2005; @Sapp2011; @Temperley1999a]. Nonetheless, the algorithm is limited by the fact that it ignores the *order* in which notes are played, despite psychological evidence that note order may indeed matter for listeners' key judgments [@brown1988; @matsunaga2005].

For illustration, consider the following two tone sequences from @matsunaga2005. Both of these sequences use exactly the same pitch classes, just in a different order, and with one of the pitch classes transposed upwards by an octave. The first sequence elicits a very clear tonal centre:

```{r, results = "asis"}
embed_image_with_audio(
  image = "images/matsunaga-tonally-clear-1.svg",
  audio = "audio/matsunaga-tonally-clear.mp3",
  width = "100%",
  title = "A tone sequence with very clear tonal implications, from @matsunaga2005."
)
```

In contrast, the second tone sequence elicits much more ambiguous tonal centre:

```{r, results = "asis"}
embed_image_with_audio(
  image = "images/matsunaga-tonally-ambiguous-1.svg",
  audio = "audio/matsunaga-tonally-ambiguous.mp3",
  width = "100%",
  title = "A tone sequence with ambiguous tonal implications, from @matsunaga2005."
)
```

@matsunaga2005 studied these two sequences alongside 58 others with the same pitch classes, and demonstrated that they elicit wide variations in key judgments. This means that the Krumhansl-Schmuckler algorithm cannot be capturing the full story, then.

Consequently, an interesting avenue of future research concerns extending the Krumhansl-Schmuckler algorithm to incorporate sensitivity to tone order. One such solution was trialled by @Toiviainen2003a, based on a dataset of relatedness judgments for pairs of tones in major and minor key contexts [@krumhansl1990cognitive]; however, this version of the algorithm failed to predict listener behaviour better than the original Krumhansl-Schmuckler algorithm. Not long later, @Madsen2007 presented a version of the Krumhansl-Schmuckler algorithm based on distributions of pairs of scale degrees as observed in music corpora, but found only marginal improvements in the algorithm's ability to recover musical keys from annotated music corpora. These failures to improve significantly upon the Krumhansl-Schmuckler algorithm are interesting, and suggest that we still have more to learn about key-finding.

### R implementation

See below for a full implementation of the Krumhansl-Schmuckler algorithm, written for the R programming language. This is provided for interest only, you are not expected to understand it, especially if you have never used R before! However, it is worth spending a while reading through the code line by line to try and get a feel for what is happening at each step. If you are confused about what a particular function does, you can click on its name to see its documentation.

To explore this code further, follow [this link](https://colab.research.google.com/drive/1tFJWKTjXs0qNR3eIXwR-sTsEuUOC1img#scrollTo=08Pjk6Oz9m85) to an online interface that allows you to edit and run the code interactively in your web browser. You will need a (free) Google account to continue; alternatively, Cambridge students can simply sign in using their Cambridge email address.

```{r, echo = TRUE}
# Load dependencies (packages which
# provide us with some useful functions).
library(dplyr)
library(purrr)

# We write out the Krumhansl-Kessler probe tone profiles.
# There are two such profiles:
# one for the major mode and one for the minor mode.
kk_profiles <- list(
  Major = c(
    6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 
    2.52, 5.19, 2.39, 3.66, 2.29, 2.88
  ),
  Minor = c(
    6.33, 2.68, 3.52, 5.38, 2.60, 3.53,
    2.54, 4.75, 3.98, 2.69, 3.34, 3.17
  )
)

# We define a function for deriving the Krumhansl-Kessler 
# profile for a given mode and tonic.
get_kk_profile <- function(mode, tonic) {
  # We begin by extracting the reference profile for that 
  # particular mode.
  ref_profile <- kk_profiles[[mode]]
  
  # We want our profile to give the fit ratings for the 
  # absolute pitch classes 0 to 11.
  absolute_pitch_classes <- 0:11
  
  # To express these pitch classes relative to the provided
  # tonic, we subtract the tonic and apply the modulo 12
  # operator (i.e. remainder after division by 12).
  relative_pitch_classes <- (absolute_pitch_classes - tonic) %% 12

  # This tells us where to look in the reference profile.
  ref_profile[1 + relative_pitch_classes]
}

# We now compile a table of all the possible key profiles,
# produced by rotating the original Krumhansl-Kessler key profiles
# through all 12 possible tonics.

# This code iterates over the numbers 0 to 11, 
# setting the tonic first to 0, then to 1, then to 2, 
# and so on...
kk_all_key_profiles <- map_dfr(0:11, function(tonic) {
  # This code iterates over the two modes (major and minor)...
  c("Major", "Minor") %>% 
    set_names(., .) %>% 
    map_dfr(function(mode) {
      # ... and gets the Krumhansl-Kessler profile for each one.
      tibble(
        tonic = tonic,
        mode = mode,
        profile = list(get_kk_profile(mode, tonic))
      )
    })
})

kk_all_key_profiles

# Here we define a function that takes a melody as an input
# and returns the estimated key.
kk_estimate_key <- function(
    pitches,  # a vector of pitches for each note in the melody
    durations # a vector of durations for each note in the melody
  ) {
  pc_distribution <- 
    tibble(pitch = pitches, duration = durations) %>%
    mutate(
      # We convert pitches into pitch classes by dividing by 12
      # and taking the remainder.
      pitch_class = factor(pitch %% 12, levels = 0:11)
    ) %>%
    
    # We sum the durations for each pich class.
    group_by(pitch_class, .drop = FALSE) %>%
    summarise(
      duration = sum(duration)
    ) %>%
    
    mutate(
      pitch_class = as.numeric(as.character(pitch_class)),
      # We normalise each duration by dividing by the total duration
      # (this is actually not strictly necessary if we are using
      # standared correlation measures).
      rel_duration = duration / sum(duration)
    )

  correlations <- 
    kk_all_key_profiles %>% 
    mutate(
      correlation = map_dbl(
                  # This function iterates over 
        profile,  # <-- the template profiles
        cor,      # <-- computing the Pearson correlation
        pc_distribution$rel_duration # <-- with the observed durations
      ))
  
  # We choose the profile that maximises the correlation 
  # with the observed distribution.
  chosen <- correlations %>% slice(which.max(correlations$correlation))
  
  chosen %>%
    select(tonic, mode, correlation) %>%
    as.list()
}

# We can use this function to estimate keys for arbitrary melodies,
# where each melody is specified as the combination of
# (a) a list of pitches and 
# (b) a list of durations.
kk_estimate_key(
  pitches =   c(60, 62, 65, 67, 69, 67), 
  durations = c(1, 0.5, 0.5, 1, 0.5, 0.5)
)

kk_estimate_key(
  pitches =   c(60, 63, 65, 67, 68, 67), 
  durations = c(1, 0.5, 0.5, 1, 0.5, 0.5)
)
```

### The Hutchinson-Knopoff dissonance algorithm

We will begin with a couple

## What role does modelling play in the scientific method?

## Modelling methodologies

Computer models are generally used for one of the three following goals: theory development, theory evaluation, and prediction generation.

### Theory development

Computer models can help us by filling in missing gaps of existing theories.

One useful application of computer models is for filling in missing gaps of theories. For example,

Suppose that we have developed a promising but incomplete theory of

Suppose that we have decided to study a particular psychological phenomenon, and suppose that we have a hypothetical

that we have a psychological phenomenon that we wish to understand, an

have identified a psychological phenomenon that we wish to understand, and we have an incomplete

This is the part of the research pipeline where we have identified a particular psychological phenomenon that we wish to study

-   we write down a model that has some missing bits

-   we use data to fill in the missing bits

    -   regression is one way of doing that

    -   parameter optimisation another (this is a generalisation of regression)

In theory development, we have identified a particular interesting psychological phenomenon, and we are trying to development

-   Optimising a memory decay hyperparameter

-   Optimising the weights of different features in an emotion model (regression)

    -   Is a regression model really a computational model? yes....?

        -   methods for developing models:

            -   rational analysis

            -   regression

-   Comparing roughness and harmonicity models

### Theory evaluation

### Theory development

to support one of three research tasks: theory development, theory evaluation, and prediction

fits into one of three stages of the research pipeline: theory development, theory evaluation, or prediction generation.

-   Theory development

-   Theory evaluation

-   Prediction generation

Computer models have two main applications in psychological research. When the problem domain is not yet well understood, computer models can play an important role in *theory development*, helping researchers to choose between competing explanations of a phenomenon. Once the problem domain becomes well-understood, computer models then become valuable for *prediction generation*, allowing the researcher to predict the outcome of an experiment without actually running it.

### Computer models for theory development

We begin this process by identifying an interesting psychological phenomenon which we wish to study. For example, we might be interested in studying listeners' perception of emotion in melodies.

The next step is to identify a set of candidate psychological mechanisms for the phenomenon. These candidate mechanisms will often be derived from previous literature, but they may also be derived from our own intuitions. These candidate mechanisms may or may not be mutually exclusive. In the case of melodic emotion perception, we might hypothesise multiple non-exclusive mechanisms, including the recognition of ascending versus descending contours, major versus minor modes, legato versus staccato articulation, and so on.

We then proceed by implementing computer models that simulate these mechanisms. Each computer model will typically be a piece of computer software that takes a musical stimulus as an input and simulates the psychological processing of that stimulus. The output will typically be one or more numbers that capture the outcome of the process. For example, a simple mode recognition algorithm might return 1 for major-key melodies and 0 for minor-key melodies.

In the next step, we design an experiment for probing these models.

For example, we might decide to study *key finding*, the process by which a listener identifies the key of a g

1.  **Identify an interesting psychological phenomenon.** For example, we might be interested in studying listeners' perception of emotion in melodies.

2.  **Identify candidate psychological mechanisms for this phenomenon.** These candidate mechanisms will often be derived from previous literature, but they may also be derived from our own intuitions. These candidate mechanisms may or may not be mutually exclusive. In the case of melodic emotion perception, we might hypothesise multiple non-exclusive mechanisms, including the recognition of ascending versus descending contours, major versus minor modes, legato versus staccato articulation, and so on.

3.  **Implement computer models that simulate these mechanisms.** Each computer model will typically be a piece of computer software that takes a musical stimulus as an input and simulates the psychological processing of that stimulus. The output will typically be one or more numbers that capture the outcome of the process. For example, a simple mode recognition algorithm might return 1 for major-key melodies and 0 for minor-key melodies.

4.  **Design an experiment for probing these models.** The experiment could simply comprise a set of musical stimuli to be presented to the participant.

5.  **Generate model predictions.** This will typically involve running the models on each stimulus and recording the models' outputs.

6.  This stimulus set should be designed to systematically probe the different computer models.

Suppose we have identified a problem domain which we want to study, for example *key finding*, the process by which a listener identifies the key of a given musical passage.

### Computer models for prediction generation

complem

Second, they can be used for *prediction generation*

When the problem domain is still not well-understood, models can be used as a tool for *theory*

## Case studies

```{r}

```

### Hutchinson & Knopoff's dissonance model

### Other models

## Modelling traditions

## Modelling and empirical research

The basic methodology of computational music psychology works as follows:

1.  **Identify an interesting psychological phenomenon.** For example, we might be interested in studying listeners' perception of emotion in melodies.

2.  **Identify candidate psychological mechanisms for this phenomenon.** These candidate mechanisms will often be derived from previous literature, but they may also be derived from our own intuitions. These candidate mechanisms may or may not be mutually exclusive. In the case of melodic emotion perception, we might hypothesise multiple non-exclusive mechanisms, including the recognition of ascending versus descending contours, major versus minor modes, legato versus staccato articulation, and so on.

3.  **Implement computer models that simulate these mechanisms.** Each computer model will typically be a piece of computer software that takes a musical stimulus as an input and simulates the psychological processing of that stimulus. The output will typically be one or more numbers that capture the outcome of the process. For example, a simple mode recognition algorithm might return 1 for major-key melodies and 0 for minor-key melodies.

4.  **Design an experiment for probing these models.** The experiment could simply comprise a set of musical stimuli to be presented to the participant.

5.  **Generate model predictions.** This will typically involve running the models on each stimulus and recording the models' outputs.

6.  This stimulus set should be designed to systematically probe the different computer models.

The basic paradigm of computational music psychology works as follows:

1.  Identify a particular psychological phenomenon of interest

2.  Compile or develop one or more candidate psychological theories for explaining that phenomenon

3.  Implement these theories as computational models.

4.  Design one or more experimental paradigms for probing these models.

5.  Generate model predictions for these experimental paradigms. If there are multiple models, these experimental paradigms should ideally yield dissociated model predictions. If not, maybe revisit the design.

6.  Compile empirical datasets from human listeners for these experimental paradigms.

7.  Compare the human responses to the model predictions.

    1.  Can compare models

    2.  Can optimise model parameters

Note: sometimes the datasets exist already.

Computational music psychologists work by developing and evaluating computer simulations of mental processes underlying musical behaviours such as music listening, music performance, and music composition. These simulations take as input ... and return as output...

The goal of ordinary music psychology research is to develop empirically grounded theories of the mental processes underlying musical behaviours such as music listening, music performance, and music composition. Typical methods include both behavioural experiments, where participants are instructed to make judgments in response to particular musical stimuli, and neuroscientific experiments, where participants have their brain responses recorded while listening to various musical stimuli.

Computational music psychology incorporates an additional component of computer simulation into this paradigm.

aim to develop effective computer simulations of the mental process

The goal of music psychology research is to understand the mental processes underlying musical behaviours such as music listening, music performance, and music composition. $$...$$ $$Typical methods include$$

Computational music psychology is a branch of music psychology that incorporates computer simulations into its workflow. These simulations are intended to represent the biological and psychological processes underpinning a particular

research

has similar goals, but incorporates

What is music psychology?

What is the role of the computer here?

Why would you use a computer here?

### Motivations

As discussed already

### Computational metaphor (information processing)

Introduce idea of data structures and algorithms

Relate to cognitive equivalents

### Traditions

-   Symbolic versus connectionist AI

-   Cognitive versus neuro

-   Psychoacoustic

### Audio vs symbolic processing

### Strategies

Intuitive:

-   Just write down what we think happens

Marr:

-   What problem is the mind trying to solve?

-   What algorithm is used to implement that solution?

-   How is that algorithm implemented in the brain?

Rational observer:

-   Derive the optimal solution to a task

-   Compare this to the brain

Bounded rationality:

-   Like rational observer approach, but incorporating processing constraints

Learning by imitation:

-   Machine learning
