# (WIP) Inferential statistics

```{r, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(
  out.width = "100%",
  echo = FALSE
)
set.seed(1)
library(tidyverse)
theme_set(theme_classic())
```

## What are inferential statistics?

Many scientific questions can be formulated in terms of one or more *populations* which we want to understand. These could be populations of humans; for example, we might wish to understand the extent to which socioeconomic status predicts educational outcomes in the general population of schoolchildren. In the context of corpus analyses, these could be populations of music compositions; for example, we might study the extent to which parallel fifths occur in Baroque compositions versus Renaissance compositions.

A given scientific experiment can rarely work with an entire population at once. It is impractical for most scientists to collect educational outcomes for all schoolchildren in the world; likewise, it would be a nigh impossible task to compile digital encodings of all Baroque and Renaissance compositions in existence.

Instead, scientists normally work with *samples*. Samples are smaller datasets that are drawn from a particular population. The hope is that the sample is representative of the larger population, and that we can learn something from the sample that can be generalised to an insight about the population as a whole.

*Inferential statistics* are a special family of statistics that are designed to help us with this generalisation task. They are intended to help us to understand when certain conclusions can be conclusively drawn from a given dataset, and when other conclusions cannot be drawn with any confidence.

## Distributions

When we talk about inferential statistics, we rely heavily on the mathematical concept of *probability distributions*. A probability distribution is a statistical model that tells us how our data samples are generated.

The most fundamental distribution in science is the *normal distribution*, also known as the *Gaussian distribution*. The normal distribution resembles a bell curve:

```{r}
x <- seq(from = -5, to = 5, by = 0.01)
y <- dnorm(x)

tibble(x, y) %>% 
  ggplot(aes(x, y)) + 
  geom_polygon(fill = "lightblue") +
  geom_line(colour = "black") +
  scale_y_continuous("Probability density")
```

The shape of this curve tells us what values are likely observations: in particular, the higher the probability density, the more likely the observation. When we generate samples from the probability distribution, they will cluster around these values.

```{r, animation.hook = 'gifski', interval = 0.33, cache = TRUE}
#| fig.caption = "A normal distribution (blue) annotated with random draws from that distribution (red vertical lines)."
x <- seq(from = -5, to = 5, by = 0.01)
y <- dnorm(x)

for (i in 1:25) {
  p <- 
    tibble(x, y) %>% 
    ggplot(aes(x, y)) + 
    geom_polygon(fill = "lightblue") +
    geom_line(colour = "black") +
    geom_vline(xintercept = rnorm(1), colour = "red") +
    scale_y_continuous("Probability density")
  print(p)
}
```

The normal distribution is defined by two parameters: its *mean* and its *standard deviation*. We encountered both of these concepts earlier in Chapter \@ref(descriptive-statistics). The normal distributions above each have a mean of 0 and a standard deviation of 1.

The mean controls the location of the normal distribution. Here are some examples of normal distributions with different means:

```{r}
means <- c(-2, 0, 2)
map_dfr(means, function(m) {
  tibble(
    x = seq(from = -10, to = 10, by = 0.01),
    y = dnorm(x, mean = m),
    mean = paste("Mean =", m)
  ) 
}) %>% 
  ggplot(
    aes(x, y, 
        colour = factor(mean), 
        fill = factor(mean))) + 
  geom_polygon(alpha = 0.2) + 
  facet_wrap(~ mean, ncol = 1) + 
  coord_cartesian(xlim = c(-4, 4)) +
  scale_x_continuous() +
  scale_y_continuous("Probability density") +
  theme(
    legend.position = "none",
    panel.spacing = unit(20, "pt"),
    strip.background = element_blank(),
    strip.text = element_text(face = "bold")
  )
```

The standard deviation (often abbreviated to 'SD") controls the spread of the normal distribution. Here are some examples with different standard deviations:

```{r}
sds <- c(0.25, 1, 2.5)
map_dfr(sds, function(sigma) {
  tibble(
    x = seq(from = -10, to = 10, by = 0.01),
    y = dnorm(x, sd = sigma),
    mean = paste("SD =", sigma)
  ) 
}) %>% 
  ggplot(
    aes(x, y, 
        colour = factor(mean), 
        fill = factor(mean))
  ) + 
  geom_polygon(alpha = 0.2) + 
  facet_wrap(~ mean, ncol = 1, scales = "free") + 
  coord_cartesian(xlim = c(-4, 4)) +
  scale_x_continuous() +
  scale_y_continuous("Probability density") +
  theme(
    legend.position = "none",
    panel.spacing = unit(20, "pt"),
    strip.background = element_blank(),
    strip.text = element_text(face = "bold")
  )
```

Much of inferential statistics can then be reduced to variants of the following logical process. We suppose that our population of interest can be modelled by some probability distribution, for example the normal distribution (which happens to resemble many real-world distributions strikingly well). We then try to *infer* the values of this distribution's parameters, for example its mean and standard deviation, based on the data we observe. 

## Sample size and uncertainty

The size of our sample is crucial for determining our ability to infer the value of a distribution's parameters. When we have only a few data values, our observations will be dominated by random chance. When we have lots of data values, however, the power of averaging will overcome the noise in the individual samples.

For illustration, let's suppose we are trying to use data to infer the mean of a normal distribution whose true value is 1.5. In the first case, we'll plot the results from 5 experiments, each of which estimate the mean based on just 10 observations:

```{r}
n_datasets <- 5

map_dfr(1:n_datasets)
```


<!-- Moreover, scientists are often interested in a more abstract conception of populations, one which is not limited to the entities that exist at the present moment, but one that also includes entities that could plausibly exist. For example, a music theorist might conceptualise the population of Bach chorale harmonisations as including not only the chorale harmonisations that Bach actually wrote, but also the chorale harmonisations that he might have written had he had the chance.  -->

-   Confidence intervals

## Linear regression

## Null hypothesis testing
